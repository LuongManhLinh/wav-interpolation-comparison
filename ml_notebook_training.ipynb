{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11899223,"sourceType":"datasetVersion","datasetId":7479969}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install soundfile torchaudio transformers datasets accelerate","metadata":{"id":"Lh9wekZWPKZ7","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:25.656731Z","iopub.execute_input":"2025-05-26T15:10:25.657647Z","iopub.status.idle":"2025-05-26T15:10:29.061840Z","shell.execute_reply.started":"2025-05-26T15:10:25.657614Z","shell.execute_reply":"2025-05-26T15:10:29.060945Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nimport os\nimport random\nfrom datasets import Dataset as HFDataset\nfrom transformers import Trainer, TrainingArguments\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom safetensors.torch import load_file","metadata":{"id":"i0eDPibxZT1l","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.063596Z","iopub.execute_input":"2025-05-26T15:10:29.063901Z","iopub.status.idle":"2025-05-26T15:10:29.069776Z","shell.execute_reply.started":"2025-05-26T15:10:29.063871Z","shell.execute_reply":"2025-05-26T15:10:29.069011Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"def split_into_chunks(waveform: torch.Tensor, chunk_size: int = 16000):\n    \"\"\"\n    Splits 1D waveform into list of fixed-size chunks (with zero-padding if needed).\n    \"\"\"\n    total_len = waveform.shape[-1]\n    pad_len = (chunk_size - total_len % chunk_size) % chunk_size\n    padded = F.pad(waveform, (0, pad_len))\n\n    chunks = padded.unfold(dimension=-1, size=chunk_size, step=chunk_size)\n    return chunks \n\n\n\nclass WaveformAutoencoder(nn.Module):\n    def __init__(self, size='small'):\n        super().__init__()\n        if size == 'small':\n            self.encoder = nn.Sequential(\n                nn.Conv1d(1, 16, 4, stride=2, padding=1),  # 8000\n                nn.LeakyReLU(),\n                nn.Conv1d(16, 32, 4, stride=2, padding=1), # 4000\n                nn.LeakyReLU(),\n                nn.Conv1d(32, 64, 4, stride=2, padding=1), # 2000\n            )\n            self.decoder = nn.Sequential(\n                nn.ConvTranspose1d(64, 32, 4, stride=2, padding=1),  # 4000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(32, 16, 4, stride=2, padding=1),  # 8000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(16, 1, 4, stride=2, padding=1),   # 16000\n                nn.Tanh()\n            )\n        elif size == 'medium':\n            self.encoder = nn.Sequential(\n                nn.Conv1d(1, 32, 4, stride=2, padding=1),  # 8000\n                nn.LeakyReLU(),\n                nn.Conv1d(32, 64, 4, stride=2, padding=1), # 4000\n                nn.LeakyReLU(),\n                nn.Conv1d(64, 128, 4, stride=2, padding=1),# 2000\n                nn.LeakyReLU(),\n                nn.Conv1d(128, 256, 4, stride=2, padding=1), # 1000\n            )\n            self.decoder = nn.Sequential(\n                nn.ConvTranspose1d(256, 128, 4, stride=2, padding=1), # 2000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(128, 64, 4, stride=2, padding=1),  # 4000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(64, 32, 4, stride=2, padding=1),   # 8000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(32, 1, 4, stride=2, padding=1),    # 16000\n                nn.Tanh()\n            )\n\n        elif size == 'large':\n            self.encoder = nn.Sequential(\n                nn.Conv1d(1, 64, 4, stride=2, padding=1),  # 8000\n                nn.LeakyReLU(),\n                nn.Conv1d(64, 128, 4, stride=2, padding=1), # 4000\n                nn.LeakyReLU(),\n                nn.Conv1d(128, 256, 4, stride=2, padding=1),# 2000\n                nn.LeakyReLU(),\n                nn.Conv1d(256, 512, 4, stride=2, padding=1), # 1000\n            )\n            self.decoder = nn.Sequential(\n                nn.ConvTranspose1d(512, 256, 4, stride=2, padding=1), # 2000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(256, 128, 4, stride=2, padding=1), # 4000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(128, 64, 4, stride=2, padding=1),   # 8000\n                nn.LeakyReLU(),\n                nn.ConvTranspose1d(64, 1, 4, stride=2, padding=1),     # 16000\n                nn.Tanh()\n            )\n        else:\n            raise ValueError(\"Invalid size. Choose from 'small', 'medium', or 'large'.\")\n    \n\n    def forward(self, **kwargs):\n        x = kwargs.get(\"input_values\")\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return {\n            \"logits\": x,\n            \"masks\": kwargs.get(\"masks\")\n        }\n    \n    @torch.no_grad()\n    def recover(self, waveform, type=\"noise\", mask=None, chunk_size=16000, device='cuda'):\n        if type not in [\"noise\", \"mask\"]:\n            raise ValueError(\"type must be either 'noise' or 'mask'\")\n        if type == \"mask\" and mask is None:\n            raise ValueError(\"mask must be provided for type 'mask'\")\n        \n        self.eval()\n        self.to(device)\n        waveform = torch.tensor(waveform)\n        print(f\"Waveform shape: {waveform.shape}\")\n\n        # Split into chunks\n        chunks = split_into_chunks(waveform, chunk_size=chunk_size)\n        print(f\"Chunks shape: {chunks.shape}\")\n        chunks = chunks.unsqueeze(1).to(device)  # add channel dimension: (N, 1, chunk_size)\n        print(f\"Chunks shape after unsqueeze: {chunks.shape}\")\n        outputs = self(input_values=chunks)[\"logits\"]\n        print(f\"Outputs shape: {outputs.shape}\")\n        # Reconstruct:\n        reconstructed = outputs.squeeze(1).reshape(-1)[:waveform.shape[-1]]\n        print(f\"Reconstructed shape: {reconstructed.shape}\")\n        \n        if type == \"noise\":\n            return reconstructed.cpu()\n        else:\n            mask = mask[-1].to(device)\n            # reconstucted if mask == 1 else waveform\n            wave_clone = waveform.clone().to(device)\n            print(\"Returning masked_reconstructed.shape\", reconstructed.shape, mask.shape, wave_clone.shape)\n            reconstructed = torch.where(mask == 1, reconstructed, wave_clone)\n            return reconstructed.cpu()\n    \n\n    def model_size_params(self):\n        \"\"\"\n        Returns the number of parameters in the model.\n        \"\"\"\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)","metadata":{"id":"FVSPTMkDZWre","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.070879Z","iopub.execute_input":"2025-05-26T15:10:29.071631Z","iopub.status.idle":"2025-05-26T15:10:29.096797Z","shell.execute_reply.started":"2025-05-26T15:10:29.071609Z","shell.execute_reply":"2025-05-26T15:10:29.096074Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport os\nimport random\n\ndef add_noise(audio, noise_prob_range, noise_var=None):\n    \"\"\"\n    Add noise to the audio tensor by randomly injecting impulses (spikes or dips).\n    \n    Args:\n        audio (torch.Tensor): The input audio tensor.\n        noise_prob_range (tuple): A tuple specifying the range of noise probabilities.\n        noise_var (float, optional): The variance of the noise to be added. Default is None.\n    Returns:\n        torch.Tensor: The noisy audio tensor.\"\"\"\n    noisy = audio.clone()\n    noise_prob = random.uniform(*noise_prob_range)  # Randomly choose noise probability\n    mask = torch.rand_like(audio) < noise_prob # binary mask: where to inject impulses\n\n    # Randomly choose 0, 1, or -1\n    impulses = torch.randint(0, 3, audio.shape).float()\n    impulses[impulses == 1] = 1.0   # spike\n    impulses[impulses == 2] = -1.0  # dip\n    impulses[impulses == 0] = 0.0   # zero-out\n\n    if noise_var:\n        noise_var = torch.randn_like(audio) * noise_var\n        noisy[mask] = impulses[mask] + noise_var[mask]\n    else:\n        noisy[mask] = impulses[mask]\n    return noisy\n\ndef mask_audio(audio, mask_prob_range, mask_value=-1):\n    \"\"\"\n    Apply a mask to the audio tensor.\n    \n    Args:\n        audio (torch.Tensor): The input audio tensor.\n        mask_prob (float): The probability of masking each element.\n        mask_value (int, optional): The value to use for masking. Default is -1\n        \n    Returns:\n        torch.Tensor: The masked audio tensor.\n    \"\"\"\n    mask_prob = random.uniform(*mask_prob_range)  # Randomly choose mask probability\n    mask = torch.rand_like(audio) < mask_prob\n    masked_audio = audio.clone()\n    masked_audio[mask] = mask_value\n    return masked_audio, mask\n    \nclass WaveformDataset(torch.utils.data.Dataset):\n    def __init__(\n            self, \n            folder, \n            chunk_length=16000, \n            sample_rate=16000, \n            type=\"noise\",\n            noise_prob_range=(0.05, 0.1), \n            noise_var=0.1,\n            mask_prob_range=(0.1, 0.2),    \n            mask_value=-100   \n        ):\n        if type not in [\"noise\", \"mask\"]:\n            raise ValueError(\"type must be either 'noise' or 'mask'\")\n        \n        self.type = type\n        self.chunk_length = chunk_length\n        self.sample_rate = sample_rate\n        self.noise_prob_range = noise_prob_range\n        self.noise_var = noise_var\n        self.audio_chunks = []\n        self.mask_prob_range = mask_prob_range\n        self.mask_value = mask_value\n        self._preload_chunks([\n            os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.wav')\n        ])\n\n    def _preload_chunks(self, file_paths):\n        for path in file_paths:\n            waveform, sr = torchaudio.load(path)\n            if sr != self.sample_rate:\n                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n                waveform = resampler(waveform)\n\n            # Convert to mono\n            if waveform.shape[0] > 1:\n                waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n            # Chunking\n            total_len = waveform.shape[1]\n            for i in range(0, total_len - self.chunk_length + 1, self.chunk_length):\n                chunk = waveform[:, i:i + self.chunk_length]\n                self.audio_chunks.append(chunk)\n\n\n    def __len__(self):\n        return len(self.audio_chunks)\n\n    def __getitem__(self, idx):\n        clean = self.audio_chunks[idx]\n        if self.type == \"noise\":\n            noisy = add_noise(clean, self.noise_prob_range, self.noise_var)\n            return {\n                \"input_values\": noisy,   # model input: noised audio\n                \"labels\": clean.clone()  # ground truth: clean audio\n            }\n        elif self.type == \"mask\":\n            masked, mask = mask_audio(clean, self.mask_prob_range, self.mask_value)\n            return {\n                \"input_values\": masked,   # model input: masked audio\n                \"labels\": clean.clone(),  # ground truth: clean audio\n                \"masks\": mask              # binary mask\n            }","metadata":{"id":"TH2tGx2sZe0C","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.097627Z","iopub.execute_input":"2025-05-26T15:10:29.097819Z","iopub.status.idle":"2025-05-26T15:10:29.115455Z","shell.execute_reply.started":"2025-05-26T15:10:29.097805Z","shell.execute_reply":"2025-05-26T15:10:29.114818Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def train(\n    model,\n    data_dir=\"audio/\",\n    chunk_length=16000,\n    sample_rate=16000,\n    type=\"noise\",  # \"noise\" or \"mask\"\n    noise_prob_range=(0.05, 0.25),\n    noise_var=0.1,\n    mask_prob_range=(0.05, 0.25),\n    batch_size=16,\n    num_epochs=1,\n    learning_rate=1e-3,\n    logging_steps=100,\n    output_dir=\"./ae_ckpt\",\n):\n    if type not in [\"noise\", \"mask\"]:\n        raise ValueError(\"type must be either 'noise' or 'mask'\")\n    \n    print(\"Loading train dataset...\")\n    audio_ds = WaveformDataset(\n        folder=data_dir, chunk_length=chunk_length, sample_rate=sample_rate, \n        noise_prob_range=noise_prob_range, noise_var=noise_var,\n        mask_prob_range=mask_prob_range, type=type\n    )\n    print(f\"Train dataset size: {len(audio_ds)}\")\n\n\n    args = TrainingArguments(\n        report_to=\"tensorboard\",\n        output_dir=output_dir,\n        per_device_train_batch_size=batch_size,\n        learning_rate=learning_rate,\n        num_train_epochs=num_epochs,\n        logging_steps=logging_steps,\n        save_strategy=\"epoch\",\n        save_total_limit=10,\n        eval_strategy=\"no\",\n        remove_unused_columns=False,\n    )\n\n    def data_collator_for_noise(batch):\n        input_values = torch.stack([item[\"input_values\"] for item in batch])\n        labels = torch.stack([item[\"labels\"] for item in batch])\n        return {\n            \"input_values\": input_values,\n            \"labels\": labels\n        }\n\n    def compute_loss_for_noise(outputs, labels, num_items_in_batch=batch_size, return_outputs=False):\n        loss = F.mse_loss(outputs[\"logits\"],labels)\n        return (loss, outputs) if return_outputs else loss\n    \n    def data_collator_for_mask(batch):\n        input_values = torch.stack([item[\"input_values\"] for item in batch])\n        labels = torch.stack([item[\"labels\"] for item in batch])\n        masks = torch.stack([item[\"masks\"] for item in batch])\n        return {\n            \"input_values\": input_values,\n            \"labels\": labels,\n            \"masks\": masks\n        }\n    \n    def compute_loss_for_mask(outputs, labels, num_items_in_batch=batch_size, return_outputs=False):\n        msk = outputs[\"masks\"]\n        logits = outputs[\"logits\"]\n        loss = F.mse_loss(logits * msk, labels * msk)\n        return (loss, outputs) if return_outputs else loss\n    \n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=audio_ds,\n        data_collator=data_collator_for_noise if type == \"noise\" else data_collator_for_mask,\n        compute_loss_func=compute_loss_for_noise if type == \"noise\" else compute_loss_for_mask\n    )\n\n    print(\"Starting training...\")\n    trainer.train(resume_from_checkpoint='/kaggle/working/autoencoder2/checkpoint-1725')","metadata":{"id":"Py4VYi7iZhia","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.117252Z","iopub.execute_input":"2025-05-26T15:10:29.117469Z","iopub.status.idle":"2025-05-26T15:10:29.135878Z","shell.execute_reply.started":"2025-05-26T15:10:29.117453Z","shell.execute_reply":"2025-05-26T15:10:29.135252Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"def compute_snr(clean_signal, reconstructed_signal):\n    signal_power = np.sum(clean_signal ** 2)\n    noise_power = np.sum((clean_signal - reconstructed_signal) ** 2)\n    \n    if noise_power == 0:\n        return np.inf  # Perfect reconstruction\n    snr = 10 * np.log10(signal_power / noise_power)\n    return snr\n\n\ndef evaluate(\n    model,\n    data_dir=\"audio/\",\n    chunk_length=16000,\n    sample_rate=16000,\n    type=\"noise\",  # \"noise\" or \"mask\"\n    noise_prob_range=(0.05, 0.25),\n    noise_var=0.1,\n    mask_prob_range=(0.05, 0.5),\n    batch_size=16,\n    device=\"cuda\"\n):\n    \"\"\"\n    Evaluate the model on the evaluation dataset.\n    \n    Args:\n        model: The model to evaluate.\n        data_dir (str): Directory containing the evaluation dataset.\n        chunk_length (int): Length of each audio chunk.\n        sample_rate (int): Sample rate of the audio.\n        type (str): Type of evaluation (\"noise\" or \"mask\").\n        noise_prob_range (tuple): Range of noise probabilities.\n        noise_var (float): Variance of the noise.\n        mask_prob (float): Probability of masking.\n        batch_size (int): Batch size for evaluation.\n        device (str): Device to use for evaluation (\"cuda\" or \"cpu\").\n    \n    Returns:\n        A dictionary containing the evaluation results.\n    \"\"\"\n    if type not in [\"noise\", \"mask\"]:\n        raise ValueError(\"type must be either 'noise' or 'mask'\")\n    \n    print(\"Loading evaluation dataset...\")\n    model.to(device)\n    model.eval()\n\n    eval_dataset = WaveformDataset(\n        data_dir, chunk_length=chunk_length, sample_rate=sample_rate, \n        noise_prob_range=noise_prob_range, noise_var=noise_var,\n        mask_prob_range=mask_prob_range, type=type)\n    \n    print(\"Loading evaluation dataset...\")\n\n    def data_collator(batch):\n        input_values = torch.stack([item[\"input_values\"] for item in batch])\n        labels = torch.stack([item[\"labels\"] for item in batch])\n        if type == \"mask\":\n            masks = torch.stack([item[\"masks\"] for item in batch])\n        else:\n            masks = None\n        return {\n            \"input_values\": input_values,\n            \"labels\": labels,\n            \"masks\": masks\n        }\n    \n        \n    dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, \n                            collate_fn=data_collator, num_workers=4)\n\n    all_clean = []\n    all_noisy = []\n    all_masks = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            input_values = batch['input_values'].to(device)\n            labels = batch['labels']\n            masks = batch['masks']\n\n            outputs = model(input_values=input_values)['logits']\n\n            all_clean.append(labels.cpu())\n            all_noisy.append(outputs.cpu())\n            if masks is not None:\n                all_masks.append(masks.cpu())\n\n    if type == \"mask\":\n        all_masks = torch.cat(all_masks, dim=0)\n\n    all_clean = torch.cat(all_clean, dim=0)\n    all_noisy = torch.cat(all_noisy, dim=0)\n\n    print(all_clean.shape)\n    print(all_noisy.shape)\n    print(all_masks.shape if type == \"mask\" else \"No masks\")\n\n    # Calculate metrics\n    mse_loss = nn.MSELoss()(all_clean * all_masks, all_noisy * all_masks).item()\n\n    print(mse_loss)","metadata":{"id":"P6dVEfmuZsld","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.136688Z","iopub.execute_input":"2025-05-26T15:10:29.136985Z","iopub.status.idle":"2025-05-26T15:10:29.156254Z","shell.execute_reply.started":"2025-05-26T15:10:29.136959Z","shell.execute_reply":"2025-05-26T15:10:29.155577Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"def recover_test(type=\"noise\",resample=False, new_sr=16000):\n    if type not in [\"noise\", \"mask\"]:\n        raise ValueError(\"type must be either 'noise' or 'mask'\")\n        \n    torch.cuda.empty_cache()\n    model = WaveformAutoencoder(size='medium')\n    state_dict = load_file(\"/kaggle/working/autoencoder/checkpoint-1725/model.safetensors\")\n    model.load_state_dict(state_dict)\n    \n    waveform, sr = torchaudio.load('/kaggle/input/audio-data/test/test_4.wav')\n    print('Raw waveform shape:', waveform.shape)\n    if waveform.shape[0] > 1:\n        waveform = torch.mean(waveform, dim=0, keepdim=True)\n    if resample:\n        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=new_sr)\n        waveform = resampler(waveform)\n        \n    print(waveform)\n    if type == \"noise\":\n        waveform = add_noise(waveform, (0.05, 0.3))\n        torchaudio.save('noised6.wav', waveform, new_sr if resample else sr)\n        waveform = waveform.squeeze(0)\n        rc = model.recover(waveform)\n        rc = rc.unsqueeze(0).to('cpu')\n        \n        print('Done')\n    else:\n        waveform, mask = mask_audio(waveform, (0.01, 0.5))\n        waveform = waveform.squeeze(0)\n        rc = model.recover(waveform, type=\"mask\", mask=mask)\n        rc = rc.unsqueeze(0).to('cpu')\n        \n    print(\"Saving recovered, shape is\", rc.shape)\n    torchaudio.save('rcv-2.wav', rc, new_sr if resample else sr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.156964Z","iopub.execute_input":"2025-05-26T15:10:29.157175Z","iopub.status.idle":"2025-05-26T15:10:29.178899Z","shell.execute_reply.started":"2025-05-26T15:10:29.157160Z","shell.execute_reply":"2025-05-26T15:10:29.178162Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"def cal_mse():\n    clean, sr = torchaudio.load('/kaggle/input/audio-data/test/test_4.wav')\n    clean = torch.mean(clean, dim=0, keepdim=True)\n    noised1, _ = torchaudio.load('/kaggle/working/rcv-2.wav')\n    noised2, _ = torchaudio.load('/kaggle/working/rcv-1.wav')\n\n    print(nn.MSELoss()(clean, noised1).item())\n    print(nn.MSELoss()(clean, noised2).item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.179773Z","iopub.execute_input":"2025-05-26T15:10:29.180012Z","iopub.status.idle":"2025-05-26T15:10:29.196842Z","shell.execute_reply.started":"2025-05-26T15:10:29.179997Z","shell.execute_reply":"2025-05-26T15:10:29.196050Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"def main():\n    model = WaveformAutoencoder(size='medium')\n    train(\n        model=model,\n        data_dir=\"/kaggle/input/audio-data/train\",\n        batch_size=256,\n        logging_steps=100,\n        type=\"mask\",\n        num_epochs=20,\n        mask_prob_range=(0.01, 0.3),\n        output_dir=\"./autoencoder3\"\n    )\n\n    evaluate(\n        model=model,\n        data_dir=\"/kaggle/input/audio-data/test\",\n        type=\"mask\"\n    )","metadata":{"id":"Y5UVAzHbZ37e","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.197606Z","iopub.execute_input":"2025-05-26T15:10:29.197801Z","iopub.status.idle":"2025-05-26T15:10:29.213793Z","shell.execute_reply.started":"2025-05-26T15:10:29.197786Z","shell.execute_reply":"2025-05-26T15:10:29.213224Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"main()\n# recover_test(type=\"mask\")\n# cal_mse()","metadata":{"id":"1f8kkjWuakKJ","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:10:29.214632Z","iopub.execute_input":"2025-05-26T15:10:29.214886Z","iopub.status.idle":"2025-05-26T15:13:42.625584Z","shell.execute_reply.started":"2025-05-26T15:10:29.214861Z","shell.execute_reply":"2025-05-26T15:13:42.624707Z"}},"outputs":[{"name":"stdout","text":"Loading train dataset...\nTrain dataset size: 29374\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2300' max='2300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2300/2300 02:29, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1800</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.001200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Loading evaluation dataset...\nLoading evaluation dataset...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 25/25 [00:00<00:00, 35.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([390, 1, 16000])\ntorch.Size([390, 1, 16000])\ntorch.Size([390, 1, 16000])\n0.0052595557644963264\n","output_type":"stream"}],"execution_count":70}]}